{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amended-machinery",
   "metadata": {},
   "source": [
    "## 1. EXPLORE THE “HOLD_OUT_VALIDATION” NOTEBOOK\n",
    "\n",
    "#### Q1. Determine where do we define all the above mentioned parameters. Observe that we run the evaluation procedure on four different problems. Each problem is a two-class two-dimensional problem, where the two sets are more and more overlapped\n",
    "\n",
    "The parameters are set as follow :\n",
    "\n",
    "N_INITS = 2\\\n",
    "N_SPLITS = 10\\\n",
    "DATASET_SIZE = 200\\\n",
    "EPOCHS = 100\\\n",
    "N_NEURONS = 2\\\n",
    "LEARNING_RATE = 0.001\\\n",
    "MOMENTUM = 0.7\\\n",
    "TRAIN_TEST_RATIO = 0.8\\\n",
    "DATA_PARAMS = np.arange(0.4, 0.71, 0.1)\\\n",
    "\n",
    "\n",
    "#### (e.g., the synthetic datasets are randomly generated using variances of 0.4, 0.5, 0.6 and 0.7).\n",
    "\n",
    "#### Q2. What are the cyan and red curves in those plots ? Why are they different ?\n",
    "\n",
    "Cyan curves are training losses, red testing losses. They are differents because we train the neural network on the training set and then we test our neural network on the testing set. Results with the testing set gives us an indication if the neural network can generalize or not to data it has not seen before.  \n",
    "\n",
    "#### Q3. What happens with the training and test errors (MSE) when we have the two sets more overlapped ?\n",
    "\n",
    "It gets bigger because if sets overlap it's no longer linearly separable and separations will mistakes some points.  \n",
    "\n",
    "#### Q4. Why sometimes the red curves indicate a higher error than the cyan ones ?\n",
    "\n",
    "Because neural network overfit by memorizing some features in the training set that might not be exactly the same in the testing set.\n",
    "\n",
    "#### Q5. What is showing the boxplot summarizing the validation errors of the preceding experiments ?\n",
    "The mean squared error of the several hold out validations for each differents data spreads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-revelation",
   "metadata": {},
   "source": [
    "## 2. EXPLORE THE “CROSS_VALIDATION” NOTEBOOK\n",
    "#### Q1. Determine where do we define all the above mentioned parameters.\n",
    "\n",
    "The parameters are set as follow : \n",
    "\n",
    "N_SPLITS = 10\\\n",
    "DATASET_SIZE = 200\\\n",
    "EPOCHS = 20\\\n",
    "N_NEURONS = 2\\\n",
    "K = 5\\\n",
    "LEARNING_RATE = 0.001\\\n",
    "MOMENTUM = 0.7\\\n",
    "DATA_PARAMS = np.arange(0.4, 0.71, 0.1)\n",
    "\n",
    "#### Q2. What is the difference between hold-out and cross-validation ? What is the new parameter that has to be defined for cross-validation ?\n",
    "Difference between hold-out and cross-validation is that with hold-out we shuffle data and then we split the data in one training and one testing set. With cross-validation we shuffle the data and do k splits of the same size of the data. We use each splits one time for testing and the other times for validation. Cross-validation ensure that each data has been used for validation. Even with several hold-out validations we are note sure that all data is used for training and validation. \n",
    "\n",
    "#### Q3. Observe the boxplots summarizing the validation errors obtained using the cross-validation method and compare them with those obtained by hold-out validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-replication",
   "metadata": {},
   "source": [
    "Standard deviation is smaller"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
